#!/usr/bin/env python3
"""
Google Docs Export Module (Phase 10-4 æ‹¡å¼µ + Enhanced JSONå¯¾å¿œ)
Enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒã‚¤ãƒ«ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªGoogle Docsã‚’ç”Ÿæˆ

ä½¿ã„æ–¹:
    from drive_docs_export import export_json_to_docs
    export_json_to_docs('path/to/file_structured_enhanced.json')

æ©Ÿèƒ½:
- Enhanced JSONã‹ã‚‰èª­ã¿ã‚„ã™ã„Google Docsä½œæˆ
- è¦ç´„ãƒ»ãƒˆãƒ”ãƒƒã‚¯ãƒ»å‚åŠ è€…ãƒ»å…¨æ–‡ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- è©±è€…åè­˜åˆ¥ä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¡¨ç¤º
- transcriptionsãƒ•ã‚©ãƒ«ãƒ€ã¸è‡ªå‹•é…ç½®
"""

import os
import json
import time
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# è¨­å®š
FOLDER_NAME = os.getenv('DRIVE_UPLOAD_FOLDER', 'transcriptions')
TOKEN_PATH = 'token.json'
# Phase 20: çµ±ä¸€ã‚¹ã‚³ãƒ¼ãƒ—ã‚’ä½¿ç”¨
SCOPES_STR = os.getenv('GOOGLE_ALL_SCOPES', 'https://www.googleapis.com/auth/drive,https://www.googleapis.com/auth/documents,https://www.googleapis.com/auth/calendar.readonly')
SCOPES = [s.strip() for s in SCOPES_STR.split(',')]


def authenticate_services():
    """
    Google Drive + Docs APIèªè¨¼
    æ—¢å­˜ã®token.jsonã‚’ä½¿ç”¨
    """
    creds = None

    if os.path.exists(TOKEN_PATH):
        creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)

    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã®å ´åˆã€ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
            # æ›´æ–°ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¿å­˜
            with open(TOKEN_PATH, 'w') as token:
                token.write(creds.to_json())
        else:
            raise ValueError("æœ‰åŠ¹ãªtoken.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚drive_download.pyã§èªè¨¼ã‚’å®Œäº†ã—ã¦ãã ã•ã„ã€‚")

    drive_service = build('drive', 'v3', credentials=creds)
    docs_service = build('docs', 'v1', credentials=creds)

    return drive_service, docs_service


def get_transcriptions_folder_id(drive_service):
    """
    transcriptionsãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—ï¼ˆæ—¢å­˜ã®drive_upload.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    """
    query = f"name='{FOLDER_NAME}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
    results = drive_service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()
    folders = results.get('files', [])

    if folders:
        return folders[0]['id']
    else:
        # ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        file_metadata = {
            'name': FOLDER_NAME,
            'mimeType': 'application/vnd.google-apps.folder'
        }
        folder = drive_service.files().create(body=file_metadata, fields='id').execute()
        print(f"âœ… ãƒ•ã‚©ãƒ«ãƒ€ '{FOLDER_NAME}' ã‚’ä½œæˆï¼ˆID: {folder['id']}ï¼‰")
        return folder['id']


def read_json_file(json_path):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def create_google_doc(docs_service, title):
    """
    ç©ºã®Google Documentã‚’ä½œæˆ
    """
    body = {
        'title': title
    }
    doc = docs_service.documents().create(body=body).execute()
    return doc['documentId']


def build_document_requests(data):
    """
    Enhanced JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Google Docs batchUpdate requestsã‚’æ§‹ç¯‰

    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹æˆ:
    1. ğŸ“Š è¦ç´„ (summary.overview)
    2. ğŸ¯ ãƒˆãƒ”ãƒƒã‚¯ (content.topics)
    3. ğŸ‘¥ å‚åŠ è€… (participants)
    4. ğŸ“ å…¨æ–‡ (segments with speaker_name)
    5. âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ  (summary.action_items)
    6. ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (summary.keywords)
    7. â„¹ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (metadata)
    """
    requests = []
    current_index = 1  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å…ˆé ­ã¯1

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³1: è¦ç´„
    overview_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“Š è¦ç´„\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    summary_data = data.get('summary', {})

    # summaryãŒæ–‡å­—åˆ—ã®å ´åˆï¼ˆæ—§å½¢å¼ï¼‰
    if isinstance(summary_data, str):
        overview = summary_data if summary_data else 'è¦ç´„ãªã—'
    # summaryãŒdictã®å ´åˆï¼ˆæ–°å½¢å¼ï¼‰
    elif isinstance(summary_data, dict):
        # summary.summary ã¾ãŸã¯ summary.overview ã‚’ä½¿ç”¨
        overview = summary_data.get('summary', summary_data.get('overview', 'è¦ç´„ãªã—'))
    else:
        overview = 'è¦ç´„ãªã—'

    overview_section += f"{overview}\n\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': overview_section
        }
    })

    # è¦ç´„è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“Š è¦ç´„\n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    current_index += len(overview_section)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: ãƒˆãƒ”ãƒƒã‚¯
    topics_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ¯ ãƒˆãƒ”ãƒƒã‚¯\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    content_data = data.get('content', {})
    topics = content_data.get('topics', [])

    if topics:
        for i, topic in enumerate(topics, 1):
            topic_name = topic.get('name', 'ãƒˆãƒ”ãƒƒã‚¯åãªã—')
            topic_summary = topic.get('summary', '')
            topic_keywords = topic.get('keywords', [])

            topics_section += f"ã€ãƒˆãƒ”ãƒƒã‚¯ {i}: {topic_name}ã€‘\n"
            if topic_summary:
                topics_section += f"{topic_summary}\n"
            if topic_keywords:
                topics_section += f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(topic_keywords)}\n"
            topics_section += "\n"
    else:
        topics_section += "ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±ãªã—\n\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': topics_section
        }
    })

    # ãƒˆãƒ”ãƒƒã‚¯è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ¯ ãƒˆãƒ”ãƒƒã‚¯\n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    current_index += len(topics_section)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³3: å‚åŠ è€…
    participants_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ‘¥ å‚åŠ è€…\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
    participants_raw = data.get('participants', [])

    # participantsãŒdictã®å ´åˆï¼ˆæ–°å½¢å¼ï¼‰
    if isinstance(participants_raw, dict):
        canonical_names = participants_raw.get('canonical_names', [])
        if canonical_names:
            for name in canonical_names:
                participants_section += f"â€¢ {name}\n"
            participants_section += "\n"
        else:
            participants_section += "å‚åŠ è€…æƒ…å ±ãªã—\n\n"
    # participantsãŒlistã®å ´åˆï¼ˆæ—§å½¢å¼ï¼‰
    elif isinstance(participants_raw, list) and participants_raw:
        for participant in participants_raw:
            if isinstance(participant, dict):
                canonical_name = participant.get('canonical_name', 'åå‰ä¸æ˜')
                organization = participant.get('organization', '')
                if organization:
                    participants_section += f"â€¢ {canonical_name} ({organization})\n"
                else:
                    participants_section += f"â€¢ {canonical_name}\n"
            elif isinstance(participant, str):
                participants_section += f"â€¢ {participant}\n"
        participants_section += "\n"
    else:
        participants_section += "å‚åŠ è€…æƒ…å ±ãªã—\n\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': participants_section
        }
    })

    # å‚åŠ è€…è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ‘¥ å‚åŠ è€…\n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    current_index += len(participants_section)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³4: å…¨æ–‡ï¼ˆè©±è€…åä»˜ãã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼‰
    transcript_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“ å…¨æ–‡\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

    segments = data.get('segments', [])
    if segments:
        for seg in segments:
            # Enhanced JSONã§ã¯speaker_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
            speaker_name = seg.get('speaker_name', seg.get('speaker', 'Unknown'))
            timestamp = seg.get('timestamp', '00:00')
            text = seg.get('text', '')
            transcript_section += f"{speaker_name} ({timestamp})\n{text}\n\n"
    else:
        transcript_section += "æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆãªã—\n\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': transcript_section
        }
    })

    # å…¨æ–‡è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“ å…¨æ–‡\n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    current_index += len(transcript_section)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³5: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
    action_items_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nâœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ \nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

    # summaryãŒdictã®å ´åˆã®ã¿action_itemsã‚’å–å¾—
    if isinstance(summary_data, dict):
        action_items = summary_data.get('action_items', [])
    else:
        action_items = []

    if action_items:
        for item in action_items:
            action_items_section += f"â€¢ {item}\n"
        action_items_section += "\n"
    else:
        action_items_section += "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ãªã—\n\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': action_items_section
        }
    })

    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nâœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ \n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    current_index += len(action_items_section)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³6: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    keywords_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

    # summaryãŒdictã®å ´åˆã®ã¿keywordsã‚’å–å¾—
    if isinstance(summary_data, dict):
        keywords = summary_data.get('keywords', [])
    else:
        keywords = []

    if keywords:
        keywords_section += f"{', '.join(keywords)}\n\n"
    else:
        keywords_section += "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãªã—\n\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': keywords_section
        }
    })

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    current_index += len(keywords_section)

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³7: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    metadata_section = "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nâ„¹ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"

    metadata = data.get('metadata', {})
    transcription_meta = metadata.get('transcription', {})
    file_meta = metadata.get('file', {})
    meeting_meta = data.get('meeting', {})

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é …ç›®
    metadata_items = []

    # ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°æƒ…å ±
    if meeting_meta.get('title'):
        metadata_items.append(f"â€¢ ã‚¿ã‚¤ãƒˆãƒ«: {meeting_meta['title']}")

    # æ–‡å­—èµ·ã“ã—æƒ…å ±
    metadata_items.extend([
        f"â€¢ æ–‡å­—èµ·ã“ã—æ—¥æ™‚: {transcription_meta.get('transcribed_at', 'N/A')}",
        f"â€¢ è¨€èª: {transcription_meta.get('language', 'N/A')}",
        f"â€¢ æ–‡å­—æ•°: {transcription_meta.get('char_count', 0):,}",
        f"â€¢ å˜èªæ•°: {transcription_meta.get('word_count', 0):,}",
        f"â€¢ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {transcription_meta.get('segment_count', 0)}",
    ])

    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
    metadata_items.extend([
        f"â€¢ ãƒ•ã‚¡ã‚¤ãƒ«å: {file_meta.get('file_name', 'N/A')}",
        f"â€¢ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_meta.get('file_size_bytes', 0) / 1024 / 1024:.2f} MB",
    ])

    # éŸ³å£°é•·ãŒã‚ã‚‹å ´åˆ
    if file_meta.get('duration_seconds'):
        duration = file_meta['duration_seconds']
        metadata_items.append(f"â€¢ éŸ³å£°é•·: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†)")

    metadata_section += "\n".join(metadata_items) + "\n"

    requests.append({
        'insertText': {
            'location': {'index': current_index},
            'text': metadata_section
        }
    })

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¦‹å‡ºã—ã®ã‚¹ã‚¿ã‚¤ãƒ«é©ç”¨
    requests.append({
        'updateParagraphStyle': {
            'range': {
                'startIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"),
                'endIndex': current_index + len("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nâ„¹ï¸ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n")
            },
            'paragraphStyle': {
                'namedStyleType': 'HEADING_1'
            },
            'fields': 'namedStyleType'
        }
    })

    return requests


def export_json_to_docs(json_path, max_retries=3):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Google Docsã‚’ä½œæˆã—ã¦transcriptionsãƒ•ã‚©ãƒ«ãƒ€ã¸é…ç½®

    Args:
        json_path: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        max_retries: ãƒªãƒˆãƒ©ã‚¤å›æ•°

    Returns:
        bool: æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
    """
    json_path = Path(json_path)

    if not json_path.exists():
        print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {json_path}")
        return False

    print(f"\nğŸ“„ Google Docsä½œæˆä¸­...")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {json_path.name}")

    try:
        # èªè¨¼
        drive_service, docs_service = authenticate_services()

        # JSONãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data = read_json_file(json_path)

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ.jsonã‚’é™¤å»ï¼‰
        doc_title = json_path.stem.replace('_structured_enhanced', '')

        # Google Docsä½œæˆ
        doc_id = create_google_doc(docs_service, doc_title)
        print(f"âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ: {doc_title}")

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŒ¿å…¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ§‹ç¯‰
        requests = build_document_requests(data)

        # batchUpdateå®Ÿè¡Œï¼ˆãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãï¼‰
        for attempt in range(max_retries):
            try:
                docs_service.documents().batchUpdate(
                    documentId=doc_id,
                    body={'requests': requests}
                ).execute()
                print(f"âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŒ¿å…¥å®Œäº†")
                break
            except HttpError as e:
                if e.resp.status == 429 and attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 2
                    print(f"âš ï¸  Rate limitï¼ˆè©¦è¡Œ {attempt + 1}/{max_retries}ï¼‰: {wait_time}ç§’å¾…æ©Ÿ...")
                    time.sleep(wait_time)
                else:
                    raise

        # transcriptionsãƒ•ã‚©ãƒ«ãƒ€ã¸ç§»å‹•
        folder_id = get_transcriptions_folder_id(drive_service)

        drive_service.files().update(
            fileId=doc_id,
            addParents=folder_id,
            fields='id, parents'
        ).execute()

        print(f"âœ… Google Docsä½œæˆå®Œäº†: {doc_title}")
        print(f"   URL: https://docs.google.com/document/d/{doc_id}/edit")
        print(f"ğŸ“± ã‚¹ãƒãƒ›ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹: Google Drive â†’ ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ– â†’ {FOLDER_NAME}")

        return True

    except HttpError as e:
        print(f"âŒ Google API ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python drive_docs_export.py <Enhanced JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("ä¾‹: python drive_docs_export.py downloads/test_structured_enhanced.json")
        sys.exit(1)

    json_path = sys.argv[1]
    success = export_json_to_docs(json_path)

    if success:
        print("\nâœ… Google Docs ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†")
    else:
        print("\nâŒ Google Docs ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¤±æ•—")
        sys.exit(1)
